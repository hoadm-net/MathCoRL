# API Usage Tracking in MathCoRL

MathCoRL b√¢y gi·ªù ƒë√£ ƒë∆∞·ª£c n√¢ng c·∫•p v·ªõi h·ªá th·ªëng tracking API usage to√†n di·ªán, cho ph√©p theo d√µi:

- **Input tokens count** - S·ªë tokens g·ª≠i ƒëi (**100% ch√≠nh x√°c** t·ª´ API metadata)
- **Output tokens count** - S·ªë tokens nh·∫≠n v·ªÅ (**100% ch√≠nh x√°c** t·ª´ API metadata)
- **Request cost** - Chi ph√≠ cho t·ª´ng request (**100% ch√≠nh x√°c** d·ª±a tr√™n actual tokens)
- **Execution time** - Th·ªùi gian th·ª±c hi·ªán request (**millisecond precision**)

### üéØ **Key Features**
- ‚úÖ **100% Accurate Token Counting** - S·ª≠ d·ª•ng official OpenAI API response metadata
- ‚úÖ **Real-time Cost Tracking** - T√≠nh to√°n chi ph√≠ ch√≠nh x√°c theo pricing m·ªõi nh·∫•t
- ‚úÖ **Comprehensive Logging** - Log t·∫•t c·∫£ API calls v·ªõi full metadata
- ‚úÖ **Multi-method Support** - Track FPP, CoT, PAL, PoT, Zero-Shot, ICRL
- ‚úÖ **Export & Analysis** - Export data ra CSV/JSON cho analysis

## üöÄ T√≠nh nƒÉng

### Automatic Tracking
T·∫•t c·∫£ API calls ƒë∆∞·ª£c t·ª± ƒë·ªông track cho c√°c ph∆∞∆°ng ph√°p:
- **FPP** (Function Prototype Prompting)
- **CoT** (Chain-of-Thought)
- **PAL** (Program-aided Language Models)
- **PoT** (Program-of-Thoughts) 
- **Zero-Shot**
- **ICRL** (Candidate Generation, Evaluation, Embedding)

### Detailed Metrics
M·ªói API call ƒë∆∞·ª£c log v·ªõi th√¥ng tin:
```json
{
  "timestamp": "2025-07-09T16:08:13.701495",
  "method": "FPP",
  "model": "gpt-4o-mini", 
  "input_tokens": 542,
  "output_tokens": 89,
  "total_tokens": 631,
  "input_cost": 0.000081,
  "output_cost": 0.000053,
  "total_cost": 0.000134,
  "execution_time": 1.23,
  "question": "What is 15 √ó 7?",
  "context": "",
  "success": true,
  "error_message": ""
}
```

### Cost Calculation
H·ªó tr·ª£ pricing t·ª± ƒë·ªông cho t·∫•t c·∫£ OpenAI models:
- **GPT-4.1-nano**: $0.0001/$0.0004 per 1K tokens
- **GPT-4.1-mini**: $0.0004/$0.0016 per 1K tokens
- **GPT-4.1**: $0.002/$0.008 per 1K tokens
- **GPT-4o-mini**: $0.00015/$0.0006 per 1K tokens
- **GPT-4o**: $0.0025/$0.01 per 1K tokens  
- **GPT-4-turbo**: $0.01/$0.03 per 1K tokens
- **GPT-3.5-turbo**: $0.0005/$0.0015 per 1K tokens
- **Embedding models**: $0.00002-$0.00013 per 1K tokens

## üìä Usage Analytics

### Real-time Tracking
```python
from mint.tracking import get_tracker

# Get usage summary
tracker = get_tracker()
summary = tracker.get_usage_summary(last_n_hours=24)

print(f"Total cost: {summary['summary']['total_cost']}")
print(f"Total tokens: {summary['summary']['total_tokens']}")
```

### Method Comparison
```python
# Compare methods by cost/efficiency
by_method = summary['by_method']
for method, stats in by_method.items():
    print(f"{method}: {stats['cost']:.6f} USD, {stats['tokens']} tokens")
```

## üîß Implementation Details

### Context Manager Pattern
```python
from mint.tracking import track_api_call

with track_api_call("CoT", "gpt-4o-mini", question, context) as tracker:
    response = llm.invoke(messages)
    tracker.set_tokens(input_tokens, output_tokens)
```

### Token Counting Workflow
```python
# 1. Pre-estimate input tokens (for preview)
input_tokens = count_tokens_openai(prompt, model_name)

# 2. Make API call
response = llm.invoke(messages)

# 3. Extract actual tokens from response metadata
actual_input_tokens, output_tokens = extract_tokens_from_response(response)

# 4. Use actual tokens if available, fallback to estimation
if actual_input_tokens > 0:
    input_tokens = actual_input_tokens  # 100% accurate

# 5. Log with precise token counts
tracker.set_tokens(input_tokens, output_tokens)
```

### Verification Example
```python
# Test ƒë·ªÉ verify token counting accuracy
from langchain_openai import ChatOpenAI
from langchain.schema import HumanMessage

llm = ChatOpenAI(model_name='gpt-4o-mini')
response = llm.invoke([HumanMessage(content='What is 2+2?')])

# Ki·ªÉm tra metadata
print(response.response_metadata['token_usage'])
# Output: {'prompt_tokens': 14, 'completion_tokens': 8, 'total_tokens': 22}
```

### Token Extraction
Tracking t·ª± ƒë·ªông extract token counts v·ªõi **ƒë·ªô ch√≠nh x√°c 100%** t·ª´:

#### 1. **API Response Metadata** (Primary - 100% Accurate)
```python
# LangChain t·ª± ƒë·ªông extract t·ª´ OpenAI API response
response_metadata.token_usage = {
    'prompt_tokens': 14,      # Input tokens (ch√≠nh x√°c 100%)
    'completion_tokens': 8,   # Output tokens (ch√≠nh x√°c 100%)
    'total_tokens': 22        # T·ªïng tokens
}
```

#### 2. **Pre-estimation** (Before API Call)
```python
# ∆Ø·ªõc t√≠nh input tokens tr∆∞·ªõc khi g·ªçi API
input_tokens = len(prompt) // 4  # ~4 characters per token
```

#### 3. **Fallback Estimation** (Rare Cases)
```python
# Ch·ªâ d√πng khi API kh√¥ng tr·∫£ metadata (hi·∫øm khi x·∫£y ra)
output_tokens = len(response.content) // 4
```

#### 4. **Accuracy Guarantee**
- ‚úÖ **Input tokens**: 100% ch√≠nh x√°c t·ª´ `prompt_tokens`
- ‚úÖ **Output tokens**: 100% ch√≠nh x√°c t·ª´ `completion_tokens`  
- ‚úÖ **Cost calculation**: 100% ch√≠nh x√°c d·ª±a tr√™n actual tokens
- ‚ö†Ô∏è **Estimation**: Ch·ªâ d√πng cho preview/fallback (~75% accuracy)

## üìÇ Log Files

### Storage Location
- **Default path**: `logs/api_usage.jsonl`
- **Format**: JSON Lines (m·ªôt JSON object per line)
- **Encoding**: UTF-8

### Log Rotation
Logs ƒë∆∞·ª£c append v√†o file hi·ªán t·∫°i. ƒê·ªÉ rotate logs:
```python
import os
from datetime import datetime

# Backup current log
current_log = "logs/api_usage.jsonl"
backup_name = f"logs/api_usage_{datetime.now().strftime('%Y%m%d_%H%M%S')}.jsonl"
os.rename(current_log, backup_name)
```

## üéØ Use Cases

### 1. Cost Monitoring
```python
# Monitor daily spending
tracker = get_tracker()
daily_stats = tracker.get_usage_summary(last_n_hours=24)
daily_cost = daily_stats['summary']['total_cost']

if daily_cost > 10.0:  # $10 threshold
    print(f"‚ö†Ô∏è Daily cost exceeded: ${daily_cost:.2f}")
```

### 2. Method Optimization
```python
# Find most efficient method
by_method = summary['by_method']
efficiency = {}
for method, stats in by_method.items():
    if stats['requests'] > 0:
        efficiency[method] = stats['cost'] / stats['requests']

best_method = min(efficiency, key=efficiency.get)
print(f"Most cost-effective: {best_method}")
```

### 3. Performance Analysis
```python
# Analyze execution times
for method, stats in by_method.items():
    avg_time = stats['time'] / stats['requests'] if stats['requests'] > 0 else 0
    print(f"{method}: {avg_time:.2f}s average")
```

## üõ† Configuration

### Custom Log Location
```python
from mint.tracking import APITracker

# Custom tracker
tracker = APITracker(log_file="custom/path/usage.jsonl")
```

### Environment Variables
```bash
# Optional: Custom model pricing
export CUSTOM_MODEL_PRICING='{"custom-model": {"input": 0.001, "output": 0.002}}'
```

## üìà Reporting

### Summary Report
```python
tracker.get_usage_summary(last_n_hours=24)
# Returns:
# {
#   "summary": {...},
#   "by_method": {...}, 
#   "by_model": {...}
# }
```

### Token Accuracy Verification
```python
# Verify tracking accuracy v·ªõi test call
from mint.tracking import get_tracker
import json

tracker = get_tracker()

# Check latest log entry
with open('logs/api_usage.jsonl', 'r') as f:
    lines = f.readlines()
    latest = json.loads(lines[-1])
    
print(f"Latest call:")
print(f"  Method: {latest['method']}")
print(f"  Model: {latest['model']}")
print(f"  Tokens: {latest['input_tokens']} ‚Üí {latest['output_tokens']}")
print(f"  Cost: ${latest['total_cost']:.6f}")
print(f"  Success: {latest['success']}")
```

### Export Data
```python
import json

# Load all tracking data
with open("logs/api_usage.jsonl", 'r') as f:
    data = [json.loads(line) for line in f if line.strip()]

# Export to CSV
import pandas as pd
df = pd.DataFrame(data)
df.to_csv("usage_report.csv", index=False)
```

## üîç Troubleshooting

### Missing Token Counts
N·∫øu token counts = 0, c√≥ th·ªÉ do:

#### 1. **API Errors**
```bash
# Quota exceeded - tokens = 0, success = false
"error_message": "Error code: 429 - You exceeded your current quota"
```

#### 2. **Network Issues**
```bash
# Connection timeout - tokens = 0, success = false  
"error_message": "Connection timeout"
```

#### 3. **Model Response Issues**
```python
# Rare case: API tr·∫£ response nh∆∞ng kh√¥ng c√≥ metadata
if not hasattr(response, 'response_metadata'):
    # Fallback to estimation
    output_tokens = len(response.content) // 4
```

#### 4. **Debug Token Counting**
```python
# Verify token extraction
from mint.tracking import extract_tokens_from_response

response = llm.invoke(messages)
input_tokens, output_tokens = extract_tokens_from_response(response)
print(f"Tokens: {input_tokens} ‚Üí {output_tokens}")

# Check response metadata
print("Has metadata:", hasattr(response, 'response_metadata'))
if hasattr(response, 'response_metadata'):
    print("Token usage:", response.response_metadata.get('token_usage'))
```

### Cost Calculation Issues
- Ki·ªÉm tra model name mapping trong `OPENAI_PRICING`
- Model m·ªõi c√≥ th·ªÉ c·∫ßn update pricing

### Log File Permissions
```bash
# Ensure logs directory is writable
chmod 755 logs/
chmod 644 logs/api_usage.jsonl
```

## üéâ Benefits

1. **Transparency**: Track exact API costs v√† usage
2. **Optimization**: Identify most efficient methods
3. **Budgeting**: Monitor v√† control spending
4. **Debugging**: Detailed error logging
5. **Analytics**: Usage patterns v√† performance metrics

---

**Note**: Tracking system ho·∫°t ƒë·ªông song song v·ªõi MathCoRL operations m√† kh√¥ng ·∫£nh h∆∞·ªüng ƒë·∫øn performance hay results. 